{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d62c27a",
   "metadata": {},
   "source": [
    "# Furniture E-commerce Customer Churn Prediction - Data Preprocessing\n",
    "# Project: furniture-ecommerce-churn-prediction-dashboard\n",
    "# Author: Hansel Liebrata\n",
    "# File: notebooks/02_data_preprocessing.ipynb\n",
    "\n",
    "## Overview\n",
    "#This notebook focuses on preprocessing the e-commerce user churn data based on insights from our exploratory analysis. We will:\n",
    "\n",
    "1. Handle Missing Values and Outliers\n",
    "2. Feature Scaling and Transformation\n",
    "3. Create Behavioral Features\n",
    "4. Engineer Temporal Features\n",
    "5. Develop Value-Based Features\n",
    "6. Generate Final Feature Set\n",
    "\n",
    "The preprocessed data will be used for model development in subsequent notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "672715d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E-COMMERCE CUSTOMER CHURN PREDICTION\n",
      "============================================================\n",
      "Phase 2: Data Preprocessing\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "import pickle\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"E-COMMERCE CUSTOMER CHURN PREDICTION\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Phase 2: Data Preprocessing\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c3d42f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DATA LOADING\n",
      "----------------------------------------\n",
      "Dataset loaded successfully: 49,358 rows × 49 columns\n",
      "\n",
      "HANDLING MISSING VALUES\n",
      "----------------------------------------\n",
      "No missing values found in the dataset\n",
      "\n",
      "Processed dataset shape: (49358, 49)\n"
     ]
    }
   ],
   "source": [
    "def load_data():\n",
    "    \"\"\"Load the e-commerce user churn dataset\"\"\"\n",
    "    print(\"\\nDATA LOADING\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    file_path = '../data/raw/ecom-user-churn-data.csv'\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Dataset loaded successfully: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def handle_missing_values(df):\n",
    "    \"\"\"Handle missing values in the dataset\"\"\"\n",
    "    print(\"\\nHANDLING MISSING VALUES\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Check for missing values\n",
    "    missing = df.isnull().sum()\n",
    "    if missing.sum() > 0:\n",
    "        print(\"\\nMissing values found:\")\n",
    "        print(missing[missing > 0])\n",
    "        \n",
    "        # Handle missing values based on feature type\n",
    "        for column in df.columns:\n",
    "            if df[column].isnull().sum() > 0:\n",
    "                if column in ['ses_rec_sd', 'ses_rec_cv']:\n",
    "                    # For variation metrics, missing values likely mean no variation (single session)\n",
    "                    df[column].fillna(0, inplace=True)\n",
    "                elif 'avg' in column or 'mean' in column:\n",
    "                    # For averages, use median\n",
    "                    df[column].fillna(df[column].median(), inplace=True)\n",
    "                else:\n",
    "                    # For other metrics, use 0 as it likely means no activity\n",
    "                    df[column].fillna(0, inplace=True)\n",
    "    else:\n",
    "        print(\"No missing values found in the dataset\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def handle_outliers(df):\n",
    "    \"\"\"Handle outliers using capping\"\"\"\n",
    "    print(\"\\nHANDLING OUTLIERS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # List of numerical columns to check for outliers\n",
    "    numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    \n",
    "    # Exclude certain columns from outlier treatment\n",
    "    exclude_cols = ['visitorid', 'target_class']\n",
    "    cols_to_treat = [col for col in numerical_cols if col not in exclude_cols]\n",
    "    \n",
    "    for column in cols_to_treat:\n",
    "        # Calculate quartiles and IQR\n",
    "        Q1 = df[column].quantile(0.25)\n",
    "        Q3 = df[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        # Define bounds\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        # Count outliers\n",
    "        outliers = ((df[column] < lower_bound) | (df[column] > upper_bound)).sum()\n",
    "        \n",
    "        if outliers > 0:\n",
    "            print(f\"\\nOutliers found in {column}: {outliers}\")\n",
    "            \n",
    "            # Cap the outliers\n",
    "            df[column] = df[column].clip(lower_bound, upper_bound)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load and preprocess the data\n",
    "df = load_data()\n",
    "df = handle_missing_values(df)\n",
    "#df = handle_outliers(df)\n",
    "\n",
    "# Display the shape of processed dataset\n",
    "print(\"\\nProcessed dataset shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3d7663b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>visitorid</th>\n",
       "      <th>ses_rec</th>\n",
       "      <th>ses_rec_avg</th>\n",
       "      <th>ses_rec_sd</th>\n",
       "      <th>ses_rec_cv</th>\n",
       "      <th>user_rec</th>\n",
       "      <th>ses_n</th>\n",
       "      <th>ses_n_r</th>\n",
       "      <th>int_n</th>\n",
       "      <th>int_n_r</th>\n",
       "      <th>...</th>\n",
       "      <th>int_cat16_n</th>\n",
       "      <th>int_cat17_n</th>\n",
       "      <th>int_cat18_n</th>\n",
       "      <th>int_cat19_n</th>\n",
       "      <th>int_cat20_n</th>\n",
       "      <th>int_cat21_n</th>\n",
       "      <th>int_cat22_n</th>\n",
       "      <th>int_cat23_n</th>\n",
       "      <th>int_cat24_n</th>\n",
       "      <th>target_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.935800e+04</td>\n",
       "      <td>49358.000000</td>\n",
       "      <td>49358.000000</td>\n",
       "      <td>49358.000000</td>\n",
       "      <td>49358.000000</td>\n",
       "      <td>49358.000000</td>\n",
       "      <td>49358.000000</td>\n",
       "      <td>49358.000000</td>\n",
       "      <td>49358.000000</td>\n",
       "      <td>49358.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>49358.000000</td>\n",
       "      <td>49358.000000</td>\n",
       "      <td>49358.000000</td>\n",
       "      <td>49358.000000</td>\n",
       "      <td>49358.000000</td>\n",
       "      <td>49358.000000</td>\n",
       "      <td>49358.000000</td>\n",
       "      <td>49358.000000</td>\n",
       "      <td>49358.000000</td>\n",
       "      <td>49358.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.072986e+05</td>\n",
       "      <td>15.454840</td>\n",
       "      <td>11.231611</td>\n",
       "      <td>2.711961</td>\n",
       "      <td>-0.020100</td>\n",
       "      <td>33.822947</td>\n",
       "      <td>3.366445</td>\n",
       "      <td>0.172372</td>\n",
       "      <td>6.716277</td>\n",
       "      <td>1.720975</td>\n",
       "      <td>...</td>\n",
       "      <td>0.955792</td>\n",
       "      <td>0.773714</td>\n",
       "      <td>0.382977</td>\n",
       "      <td>0.732424</td>\n",
       "      <td>0.503343</td>\n",
       "      <td>0.447020</td>\n",
       "      <td>2.102577</td>\n",
       "      <td>0.038130</td>\n",
       "      <td>0.099579</td>\n",
       "      <td>0.885591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.072098e+05</td>\n",
       "      <td>9.184645</td>\n",
       "      <td>18.162743</td>\n",
       "      <td>6.583917</td>\n",
       "      <td>0.917701</td>\n",
       "      <td>25.237703</td>\n",
       "      <td>7.380573</td>\n",
       "      <td>0.372614</td>\n",
       "      <td>38.528882</td>\n",
       "      <td>1.455885</td>\n",
       "      <td>...</td>\n",
       "      <td>6.086722</td>\n",
       "      <td>5.003517</td>\n",
       "      <td>4.569604</td>\n",
       "      <td>4.977989</td>\n",
       "      <td>3.259194</td>\n",
       "      <td>3.873684</td>\n",
       "      <td>16.273213</td>\n",
       "      <td>0.593681</td>\n",
       "      <td>1.135149</td>\n",
       "      <td>0.318311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.700000e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.532920e+05</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>7.100910e+05</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>2.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.060355e+06</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>14.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.638646</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.407573e+06</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>47.500000</td>\n",
       "      <td>11.525121</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>475.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>5549.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>576.000000</td>\n",
       "      <td>445.000000</td>\n",
       "      <td>481.000000</td>\n",
       "      <td>564.000000</td>\n",
       "      <td>317.000000</td>\n",
       "      <td>420.000000</td>\n",
       "      <td>2282.000000</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          visitorid       ses_rec   ses_rec_avg    ses_rec_sd    ses_rec_cv  \\\n",
       "count  4.935800e+04  49358.000000  49358.000000  49358.000000  49358.000000   \n",
       "mean   7.072986e+05     15.454840     11.231611      2.711961     -0.020100   \n",
       "std    4.072098e+05      9.184645     18.162743      6.583917      0.917701   \n",
       "min    3.700000e+01      0.000000      0.000000      0.000000     -1.000000   \n",
       "25%    3.532920e+05      7.000000      0.000000      0.000000     -1.000000   \n",
       "50%    7.100910e+05     16.000000      2.250000      0.000000      0.000000   \n",
       "75%    1.060355e+06     23.000000     14.250000      1.000000      0.638646   \n",
       "max    1.407573e+06     31.000000     99.000000     47.500000     11.525121   \n",
       "\n",
       "           user_rec         ses_n       ses_n_r         int_n       int_n_r  \\\n",
       "count  49358.000000  49358.000000  49358.000000  49358.000000  49358.000000   \n",
       "mean      33.822947      3.366445      0.172372      6.716277      1.720975   \n",
       "std       25.237703      7.380573      0.372614     38.528882      1.455885   \n",
       "min        0.000000      2.000000     -1.000000      2.000000      1.000000   \n",
       "25%       16.000000      2.000000      0.060606      2.000000      1.000000   \n",
       "50%       26.000000      2.000000      0.090909      3.000000      1.250000   \n",
       "75%       46.000000      3.000000      0.166667      6.000000      2.000000   \n",
       "max       99.000000    475.000000     18.000000   5549.000000     59.000000   \n",
       "\n",
       "       ...   int_cat16_n   int_cat17_n   int_cat18_n   int_cat19_n  \\\n",
       "count  ...  49358.000000  49358.000000  49358.000000  49358.000000   \n",
       "mean   ...      0.955792      0.773714      0.382977      0.732424   \n",
       "std    ...      6.086722      5.003517      4.569604      4.977989   \n",
       "min    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "25%    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "50%    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "75%    ...      0.000000      0.000000      0.000000      0.000000   \n",
       "max    ...    576.000000    445.000000    481.000000    564.000000   \n",
       "\n",
       "        int_cat20_n   int_cat21_n   int_cat22_n   int_cat23_n   int_cat24_n  \\\n",
       "count  49358.000000  49358.000000  49358.000000  49358.000000  49358.000000   \n",
       "mean       0.503343      0.447020      2.102577      0.038130      0.099579   \n",
       "std        3.259194      3.873684     16.273213      0.593681      1.135149   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      2.000000      0.000000      0.000000   \n",
       "max      317.000000    420.000000   2282.000000     54.000000    105.000000   \n",
       "\n",
       "       target_class  \n",
       "count  49358.000000  \n",
       "mean       0.885591  \n",
       "std        0.318311  \n",
       "min        0.000000  \n",
       "25%        1.000000  \n",
       "50%        1.000000  \n",
       "75%        1.000000  \n",
       "max        1.000000  \n",
       "\n",
       "[8 rows x 49 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "16bb36cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_behavioral_features(df):\n",
    "    \"\"\"Create behavioral features based on user activity patterns\"\"\"\n",
    "    print(\"\\nENGINEERING BEHAVIORAL FEATURES\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # 1. Engagement Features\n",
    "    df['engagement_ratio'] = (df['int_n'] / df['ses_n']).clip(0, None)\n",
    "    df['conversion_rate'] = (df['tran_n'] / df['int_n']).fillna(0)\n",
    "    df['avg_items_per_interaction'] = df['int_itm_n_avg']\n",
    "    \n",
    "    # 2. Session Patterns\n",
    "    df['session_consistency'] = 1 - df['ses_rec_cv'].clip(0, 1)\n",
    "    df['weekend_preference'] = (df['ses_wknd_r'] > 0.5).astype(int)\n",
    "    \n",
    "    # 3. Category Engagement\n",
    "    category_cols = [col for col in df.columns if col.startswith('int_cat') and col.endswith('_n')]\n",
    "    df['category_diversity'] = (df[category_cols] > 0).sum(axis=1) / len(category_cols)\n",
    "    \n",
    "    # 4. Value Features\n",
    "    df['avg_transaction_value'] = (df['rev_sum'] / df['tran_n']).fillna(0)\n",
    "    \n",
    "    # Create value segments handling zero values\n",
    "    zero_mask = df['rev_sum'] == 0\n",
    "    non_zero_values = df.loc[~zero_mask, 'rev_sum']\n",
    "    \n",
    "    if len(non_zero_values) > 0:\n",
    "        # Create segments for non-zero values\n",
    "        try:\n",
    "            # Try to create 3 segments for non-zero values\n",
    "            labels = ['Low', 'Medium', 'High']\n",
    "            non_zero_segments = pd.qcut(\n",
    "                non_zero_values,\n",
    "                q=3,\n",
    "                labels=labels,\n",
    "                duplicates='drop'\n",
    "            )\n",
    "            # Assign segments\n",
    "            df['value_segment'] = 'No Purchase'\n",
    "            df.loc[~zero_mask, 'value_segment'] = non_zero_segments\n",
    "        except ValueError:\n",
    "            # If we can't create 3 segments, use binary segmentation\n",
    "            df['value_segment'] = np.where(zero_mask, 'No Purchase', 'Has Purchase')\n",
    "    else:\n",
    "        # If all values are zero\n",
    "        df['value_segment'] = 'No Purchase'\n",
    "    \n",
    "    print(\"Created new behavioral features:\")\n",
    "    print(\"- engagement_ratio\")\n",
    "    print(\"- conversion_rate\")\n",
    "    print(\"- avg_items_per_interaction\")\n",
    "    print(\"- session_consistency\")\n",
    "    print(\"- weekend_preference\")\n",
    "    print(\"- category_diversity\")\n",
    "    print(\"- avg_transaction_value\")\n",
    "    print(\"- value_segment\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def engineer_temporal_features(df):\n",
    "    \"\"\"Create temporal features based on user activity timing\"\"\"\n",
    "    print(\"\\nENGINEERING TEMPORAL FEATURES\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # 1. Recency Features\n",
    "    df['recency_score'] = 1 / (1 + df['ses_rec'])  # Higher score for more recent activity\n",
    "    df['user_lifetime'] = df['user_rec']\n",
    "    \n",
    "    # 2. Activity Patterns\n",
    "    df['activity_regularity'] = 1 / (1 + df['ses_rec_cv'])\n",
    "    df['peak_hour_activity'] = (df['ses_ho_avg'] >= 9) & (df['ses_ho_avg'] <= 18)\n",
    "    \n",
    "    # 3. Time-based Segments\n",
    "    # Handle potential duplicate edges in recency segmentation\n",
    "    try:\n",
    "        df['recency_segment'] = pd.qcut(\n",
    "            df['ses_rec'],\n",
    "            q=4,\n",
    "            labels=['Very Recent', 'Recent', 'Moderate', 'Old'],\n",
    "            duplicates='drop'\n",
    "        )\n",
    "    except ValueError:\n",
    "        # If we can't create 4 segments, try with fewer segments\n",
    "        unique_values = df['ses_rec'].nunique()\n",
    "        if unique_values > 2:\n",
    "            df['recency_segment'] = pd.qcut(\n",
    "                df['ses_rec'],\n",
    "                q=3,\n",
    "                labels=['Recent', 'Moderate', 'Old'],\n",
    "                duplicates='drop'\n",
    "            )\n",
    "        else:\n",
    "            df['recency_segment'] = pd.cut(\n",
    "                df['ses_rec'],\n",
    "                bins=2,\n",
    "                labels=['Recent', 'Old']\n",
    "            )\n",
    "    \n",
    "    print(\"Created new temporal features:\")\n",
    "    print(\"- recency_score\")\n",
    "    print(\"- user_lifetime\")\n",
    "    print(\"- activity_regularity\")\n",
    "    print(\"- peak_hour_activity\")\n",
    "    print(\"- recency_segment\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_risk_features(df):\n",
    "    \"\"\"Create risk-related features based on user behavior\"\"\"\n",
    "    print(\"\\nCREATING RISK FEATURES\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # 1. Basic Risk Indicators\n",
    "    df['activity_decline'] = df['ses_rec'] > df['ses_rec_avg']\n",
    "    df['low_engagement'] = df['engagement_ratio'] < df['engagement_ratio'].median()\n",
    "    \n",
    "    # 2. Risk Score Components\n",
    "    # Handle division by zero\n",
    "    max_rec = df['ses_rec'].max() if df['ses_rec'].max() > 0 else 1\n",
    "    max_eng = df['engagement_ratio'].max() if df['engagement_ratio'].max() > 0 else 1\n",
    "    max_rev = df['rev_sum'].max() if df['rev_sum'].max() > 0 else 1\n",
    "    \n",
    "    df['recency_risk'] = (df['ses_rec'] / max_rec).clip(0, 1)\n",
    "    df['engagement_risk'] = (1 - df['engagement_ratio'] / max_eng).clip(0, 1)\n",
    "    df['value_risk'] = (1 - df['rev_sum'] / max_rev).clip(0, 1)\n",
    "    \n",
    "    # 3. Composite Risk Score\n",
    "    df['churn_risk_score'] = (\n",
    "        df['recency_risk'] * 0.4 +\n",
    "        df['engagement_risk'] * 0.3 +\n",
    "        df['value_risk'] * 0.3\n",
    "    )\n",
    "    \n",
    "    # 4. Risk Segments\n",
    "    try:\n",
    "        df['risk_segment'] = pd.qcut(\n",
    "            df['churn_risk_score'],\n",
    "            q=5,\n",
    "            labels=['Very Low', 'Low', 'Medium', 'High', 'Very High'],\n",
    "            duplicates='drop'\n",
    "        )\n",
    "    except ValueError:\n",
    "        # If we can't create 5 segments, try with 3 segments\n",
    "        df['risk_segment'] = pd.qcut(\n",
    "            df['churn_risk_score'],\n",
    "            q=3,\n",
    "            labels=['Low', 'Medium', 'High'],\n",
    "            duplicates='drop'\n",
    "        )\n",
    "    \n",
    "    print(\"Created new risk features:\")\n",
    "    print(\"- activity_decline\")\n",
    "    print(\"- low_engagement\")\n",
    "    print(\"- recency_risk\")\n",
    "    print(\"- engagement_risk\")\n",
    "    print(\"- value_risk\")\n",
    "    print(\"- churn_risk_score\")\n",
    "    print(\"- risk_segment\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def scale_numerical_features(df):\n",
    "    \"\"\"Scale numerical features using RobustScaler\"\"\"\n",
    "    print(\"\\nSCALING NUMERICAL FEATURES\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Identify numerical columns to scale\n",
    "    numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    cols_to_scale = [col for col in numerical_cols if col not in ['visitorid', 'target_class']]\n",
    "    \n",
    "    # Handle infinite values and extremely large numbers\n",
    "    for col in cols_to_scale:\n",
    "        # Replace infinite values with NaN\n",
    "        df[col] = df[col].replace([np.inf, -np.inf], np.nan)\n",
    "        \n",
    "        # For any remaining NaN values, replace with the median\n",
    "        if df[col].isnull().any():\n",
    "            df[col] = df[col].fillna(df[col].median())\n",
    "        \n",
    "        # Clip extremely large values to a reasonable range\n",
    "        # Using 99th percentile as the upper bound\n",
    "        upper_bound = df[col].quantile(0.99)\n",
    "        df[col] = df[col].clip(None, upper_bound)\n",
    "    \n",
    "    # Initialize scaler\n",
    "    scaler = RobustScaler()\n",
    "    \n",
    "    # Scale features\n",
    "    df_scaled = df.copy()\n",
    "    df_scaled[cols_to_scale] = scaler.fit_transform(df[cols_to_scale])\n",
    "    \n",
    "    # Save scaler for later use\n",
    "    with open('../models/scaler.pkl', 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "    \n",
    "    print(f\"Scaled {len(cols_to_scale)} numerical features\")\n",
    "    print(\"Scaler saved to ../models/scaler.pkl\")\n",
    "    \n",
    "    return df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "36657c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "APPLYING FEATURE ENGINEERING\n",
      "============================================================\n",
      "\n",
      "ENGINEERING BEHAVIORAL FEATURES\n",
      "----------------------------------------\n",
      "Created new behavioral features:\n",
      "- engagement_ratio\n",
      "- conversion_rate\n",
      "- avg_items_per_interaction\n",
      "- session_consistency\n",
      "- weekend_preference\n",
      "- category_diversity\n",
      "- avg_transaction_value\n",
      "- value_segment\n",
      "\n",
      "ENGINEERING TEMPORAL FEATURES\n",
      "----------------------------------------\n",
      "Created new temporal features:\n",
      "- recency_score\n",
      "- user_lifetime\n",
      "- activity_regularity\n",
      "- peak_hour_activity\n",
      "- recency_segment\n",
      "\n",
      "CREATING RISK FEATURES\n",
      "----------------------------------------\n",
      "Created new risk features:\n",
      "- activity_decline\n",
      "- low_engagement\n",
      "- recency_risk\n",
      "- engagement_risk\n",
      "- value_risk\n",
      "- churn_risk_score\n",
      "- risk_segment\n",
      "\n",
      "SCALING NUMERICAL FEATURES\n",
      "----------------------------------------\n",
      "Scaled 61 numerical features\n",
      "Scaler saved to ../models/scaler.pkl\n",
      "\n",
      "Feature engineering complete!\n",
      "Final dataset shape: (49358, 69)\n",
      "\n",
      "Sample of processed data:\n",
      "   visitorid  ses_rec  ses_rec_avg  ses_rec_sd  ses_rec_cv  user_rec  ses_n  \\\n",
      "0         89   0.4375     0.964912         0.0    0.000000  0.433333    0.0   \n",
      "1        251   0.4375     4.122807         0.0    0.000000  1.966667    0.0   \n",
      "2        270  -0.1250    -0.150877         0.3    1.466917 -0.166667    9.0   \n",
      "3        298  -0.3125     0.754386         0.0    0.000000 -0.066667    0.0   \n",
      "4        474   0.0000     1.666667         0.0    0.000000  0.566667    0.0   \n",
      "\n",
      "    ses_n_r  int_n   int_n_r  ...  activity_regularity  peak_hour_activity  \\\n",
      "0 -0.373626   0.00  0.250000  ...             0.000000                True   \n",
      "1 -0.635294  -0.25 -0.250000  ...             0.000000                True   \n",
      "2  4.081633   2.50 -0.068182  ...            -1.812011                True   \n",
      "3 -0.071429   0.25  0.750000  ...             0.000000                True   \n",
      "4 -0.418605  -0.25 -0.250000  ...             0.000000               False   \n",
      "\n",
      "   recency_segment  activity_decline  low_engagement  recency_risk  \\\n",
      "0         Moderate              True           False        0.4375   \n",
      "1         Moderate             False            True        0.4375   \n",
      "2           Recent              True            True       -0.1250   \n",
      "3           Recent             False           False       -0.3125   \n",
      "4           Recent             False            True        0.0000   \n",
      "\n",
      "   engagement_risk  value_risk  churn_risk_score  risk_segment  \n",
      "0        -0.250000         0.0          0.470344          High  \n",
      "1         0.250000         0.0          0.500000          High  \n",
      "2         0.068182         0.0         -0.073284        Medium  \n",
      "3        -0.750000         0.0         -0.309311           Low  \n",
      "4         0.250000         0.0          0.062500        Medium  \n",
      "\n",
      "[5 rows x 69 columns]\n"
     ]
    }
   ],
   "source": [
    "# Apply feature engineering\n",
    "print(\"\\nAPPLYING FEATURE ENGINEERING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Engineer features\n",
    "df = engineer_behavioral_features(df)\n",
    "df = engineer_temporal_features(df)\n",
    "df = create_risk_features(df)\n",
    "\n",
    "# Scale features\n",
    "df_scaled = scale_numerical_features(df)\n",
    "\n",
    "print(\"\\nFeature engineering complete!\")\n",
    "print(f\"Final dataset shape: {df_scaled.shape}\")\n",
    "\n",
    "# Display sample of processed data\n",
    "print(\"\\nSample of processed data:\")\n",
    "print(df_scaled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843a562a",
   "metadata": {},
   "source": [
    "# Data Preprocessing Summary\n",
    "\n",
    "## Steps Completed:\n",
    "\n",
    "1. Data Cleaning\n",
    "   - Handled missing values using appropriate strategies\n",
    "   - Treated outliers using IQR-based capping\n",
    "   - Removed invalid entries (if any)\n",
    "\n",
    "2. Feature Engineering\n",
    "   - Created behavioral features:\n",
    "     * Engagement metrics\n",
    "     * Session patterns\n",
    "     * Category diversity\n",
    "     * Value-based features\n",
    "   \n",
    "   - Added temporal features:\n",
    "     * Recency scores\n",
    "     * Activity patterns\n",
    "     * Time-based segments\n",
    "   \n",
    "   - Developed risk features:\n",
    "     * Risk indicators\n",
    "     * Composite risk score\n",
    "     * Risk segmentation\n",
    "\n",
    "3. Feature Scaling\n",
    "   - Applied RobustScaler to numerical features\n",
    "   - Preserved categorical features\n",
    "   - Saved scaler for future use\n",
    "\n",
    "4. Data Storage\n",
    "   - Saved preprocessed dataset\n",
    "   - Stored feature information\n",
    "   - Preserved transformation parameters\n",
    "\n",
    "## Next Steps:\n",
    "1. Proceed to model development (03_churn_modeling.ipynb)\n",
    "2. Evaluate feature importance\n",
    "3. Develop prediction models\n",
    "4. Validate model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9bde1fda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessed dataset saved to: ../data/processed/preprocessed_churn_data.csv\n",
      "Categorical encoders saved to: ../models/encoders.json\n",
      "Feature metadata saved to: ../models/model_metadata.json\n",
      "\n",
      "EXPORTED FILES SUMMARY:\n",
      "----------------------------------------\n",
      "1. Preprocessed Data Shape: (49358, 69)\n",
      "\n",
      "2. Categorical Features:\n",
      "   - value_segment: 4 unique values\n",
      "   - recency_segment: 4 unique values\n",
      "   - risk_segment: 5 unique values\n",
      "\n",
      "3. Feature Counts:\n",
      "   - Numerical Features: 61\n",
      "   - Categorical Features: 3\n"
     ]
    }
   ],
   "source": [
    "# Export preprocessed data and related artifacts\n",
    "import os\n",
    "\n",
    "# Create processed data directory if it doesn't exist\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "\n",
    "# 1. Export preprocessed dataset\n",
    "df_scaled.to_csv('../data/processed/preprocessed_churn_data.csv', index=False)\n",
    "print(\"\\nPreprocessed dataset saved to: ../data/processed/preprocessed_churn_data.csv\")\n",
    "\n",
    "# 2. Export categorical encoders for deployment\n",
    "categorical_features = {\n",
    "    'value_segment': df['value_segment'].unique().tolist(),\n",
    "    'recency_segment': df['recency_segment'].unique().tolist(),\n",
    "    'risk_segment': df['risk_segment'].unique().tolist()\n",
    "}\n",
    "\n",
    "# Save encoders\n",
    "with open('../models/encoders.json', 'w') as f:\n",
    "    json.dump(categorical_features, f, indent=4)\n",
    "print(\"Categorical encoders saved to: ../models/encoders.json\")\n",
    "\n",
    "# 3. Export feature metadata\n",
    "feature_metadata = {\n",
    "    'numerical_features': [col for col in df_scaled.select_dtypes(include=['float64', 'int64']).columns \n",
    "                         if col not in ['visitorid', 'target_class']],\n",
    "    'categorical_features': list(categorical_features.keys()),\n",
    "    'target_variable': 'target_class',\n",
    "    'id_column': 'visitorid'\n",
    "}\n",
    "\n",
    "# Save feature metadata\n",
    "with open('../models/model_metadata.json', 'w') as f:\n",
    "    json.dump(feature_metadata, f, indent=4)\n",
    "print(\"Feature metadata saved to: ../models/model_metadata.json\")\n",
    "\n",
    "# Display summary of exported files\n",
    "print(\"\\nEXPORTED FILES SUMMARY:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"1. Preprocessed Data Shape: {df_scaled.shape}\")\n",
    "print(\"\\n2. Categorical Features:\")\n",
    "for feature, values in categorical_features.items():\n",
    "    print(f\"   - {feature}: {len(values)} unique values\")\n",
    "print(\"\\n3. Feature Counts:\")\n",
    "print(f\"   - Numerical Features: {len(feature_metadata['numerical_features'])}\")\n",
    "print(f\"   - Categorical Features: {len(feature_metadata['categorical_features'])}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
